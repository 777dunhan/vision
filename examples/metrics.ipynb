{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# load and standardize data\n",
    "\n",
    "import brainscore\n",
    "\n",
    "neural_data = brainscore.get_assembly(name=\"dicarlo.Majaj2015\")\n",
    "neural_data.load()\n",
    "neural_data = neural_data.sel(variation=6).multi_groupby(['category_name', 'object_name', 'image_id']) \\\n",
    "    .mean(dim='presentation').squeeze('time_bin').T\n",
    "# Mostly, we compare neural data with computational models, \n",
    "# for deep neural networks see https://github.com/mschrimpf/brain-score-models.\n",
    "# This repository is agnostic of the comparison system, \n",
    "# To show-case the functionality, we are going to compare different regions.\n",
    "v4_data = neural_data.sel(region='V4')\n",
    "it_data = neural_data.sel(region='IT')\n",
    "\n",
    "# We can compare a set of assemblies directly by calling the metric \n",
    "# but for more sophisticated comparisons we will usually build a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per neuroid:  <xarray.DataAssembly (neuroid_id: 10)>\narray([0.431195, 0.5617  , 0.554427, 0.554573, 0.551613, 0.446308, 0.500028,\n       0.562305, 0.551316, 0.472134])\nCoordinates:\n  * neuroid_id  (neuroid_id) object 'Chabo_L_M_5_9' 'Chabo_L_M_6_9' ... ...\n\nmedian over neuroids: <xarray.DataAssembly ()>\narray(0.55406)\n"
     ]
    }
   ],
   "source": [
    "# To compare two systems, we instantiate the metric and call it on the source and target assembly.\n",
    "# The neural fit also relies on training data to instantiate the regression.\n",
    "from brainscore.metrics.neural_fit import NeuralFit\n",
    "\n",
    "neural_fit = NeuralFit()\n",
    "# For demonstration purposes, we will use the same data for training and testing. \n",
    "# (which you should obviously never do in practice)\n",
    "score = neural_fit(v4_data, it_data, v4_data, it_data)\n",
    "# This gives us a score, containing the correlations per neuroid. \n",
    "# For instance, there is one value per cross-validation split.\n",
    "print(\"per neuroid: \", score[:10], \"...\\n\")\n",
    "# Usually we want to aggregate over neuroids to yield a single scalar value:\n",
    "aggregate = neural_fit.aggregate(score)\n",
    "print(\"median over neuroids:\", aggregate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataAssembly ()>\narray(0.289892)\n"
     ]
    }
   ],
   "source": [
    "# We can easily swap out the specific metric and use e.g. RDMs.\n",
    "# To compare two systems, we instantiate the metric and call it on the source and target assembly.\n",
    "from brainscore.metrics.rdm import RDMMetric\n",
    "\n",
    "rdm = RDMMetric()\n",
    "score = rdm(v4_data, it_data)\n",
    "print(score)\n",
    "# Note how the score is much lower with RDMs due to missing re-mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stratification coord 'None' not found in assembly - falling back to un-stratified splits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stratification coord 'None' not found in assembly - falling back to un-stratified splits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw score values:  <xarray.DataAssembly (region: 2, split: 10, neuroid_id: 256)>\narray([[[1.020304, 0.953583, ...,      nan,      nan],\n        [1.143224, 0.99301 , ...,      nan,      nan],\n        ...,\n        [1.065376, 0.924344, ...,      nan,      nan],\n        [1.000426, 0.966133, ...,      nan,      nan]],\n\n       [[     nan,      nan, ..., 0.867279, 0.885616],\n        [     nan,      nan, ..., 0.875509, 0.900431],\n        ...,\n        [     nan,      nan, ..., 0.845986, 0.878324],\n        [     nan,      nan, ..., 0.856884, 0.994246]]])\nCoordinates:\n  * neuroid_id  (neuroid_id) object 'Chabo_L_A_2_4' 'Chabo_L_A_3_3' ...\n  * region      (region) object 'IT' 'V4'\n  * split       (split) int64 0 1 2 3 4 5 6 7 8 9 \n\ncenter: <xarray.DataAssembly (region: 2)>\narray([1.011786, 0.686611])\nCoordinates:\n    aggregation  <U6 'center'\n  * region       (region) object 'IT' 'V4'\n"
     ]
    }
   ],
   "source": [
    "# So far, we have compared data directly and gotten a single score.\n",
    "# But we are not sure how reliable this score is. \n",
    "# And we might want to compare with V4 and IT data jointly without having to split them manually.\n",
    "# This is what Benchmarks are for: they automatically compare multiple streams of data and cross-validate.\n",
    "# We can access pre-defined benchmarks by their name:\n",
    "\n",
    "from brainscore import benchmarks\n",
    "\n",
    "benchmark = benchmarks.load('dicarlo.Majaj2015')\n",
    "# And then just pass in the assembly to compare:\n",
    "score = benchmark(it_data)\n",
    "# This gives us a score, containing the raw `values` that went into the score. \n",
    "# For instance, there is one value per cross-validation split.\n",
    "print(\"raw score values: \", score.values, \"\\n\")\n",
    "# We often care only about a value and error bars, \n",
    "# so the score also contains an `aggregation` with a center and error (typically mean and s.e.m.).\n",
    "# Since the score is potentially multi-dimensional, it is also structured as an xarray DataArray.\n",
    "# We can select the center as follows:\n",
    "print(\"center:\", score.aggregation.sel(aggregation='center'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also define our own benchmarks.\n",
    "# If you just want to put together existing assemblies and metrics in new ways, use the `build` method:\n",
    "my_benchmark = benchmarks.build(assembly_name='dicarlo.Majaj2015', metric_name='rdm', target_splits=['subregion'])\n",
    "# If you want to use new data, a new metric etc., you should call the benchmark constructor.\n",
    "# Here, we want to keep the transformations (i.e. cross-validation), so we'll use SplitBenchmark:\n",
    "\n",
    "from brainscore import benchmarks\n",
    "from brainscore.benchmarks import SplitBenchmark\n",
    "from brainscore.metrics.neural_fit import NeuralFit\n",
    "from brainscore.metrics.ceiling import ceilings\n",
    "\n",
    "assembly_loader = benchmarks._assemblies['dicarlo.Majaj2015']\n",
    "assembly = assembly_loader(average_repetition=False).sel(region='IT')\n",
    "metric = NeuralFit()\n",
    "ceiling = ceilings['splitrep'](metric, average_repetition=assembly_loader.average_repetition)\n",
    "my_benchmark = SplitBenchmark(target_assembly=assembly, metric=metric, ceiling=ceiling, target_splits=['region'])"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "mkgu"
  },
  "kernelspec": {
   "display_name": "mkgu",
   "language": "python",
   "name": "mkgu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nteract": {
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
